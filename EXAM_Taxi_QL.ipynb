{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Description du jeu</h3>\n",
    "\n",
    "<h5>ELEMENTS</h5>\n",
    "En fonction du jeu : location du passager ou de la destination:\n",
    "\n",
    "- bleu\n",
    "\n",
    "- rouge\n",
    "\n",
    "- jaune\n",
    "\n",
    "- vert\n",
    "\n",
    "Autres lieux: lieux divers\n",
    "\n",
    "\n",
    "<h5>ACTIONS</h5>\n",
    "\n",
    "- 0: déplacer vers le Sud (bas)\n",
    "- 1: déplacer vers le Nord (haut)\n",
    "- 2: déplacer vers l'Est (gauche) \n",
    "- 3: déplacer vers le Ouest (droite) \n",
    "- 4: prendre passager\n",
    "- 5: déposer passager\n",
    "\n",
    "\n",
    "<h5>RECOMPENSES</h5> \n",
    "\n",
    "- DONE (fin de jeu) : 20\n",
    "\n",
    "- Déposer le passager à la mauvaise destination : -10\n",
    "\n",
    "- Prendre un autre passager que le passager bleu : -10\n",
    "\n",
    "- Pour toute autre action, parmi les actions de 0 -> 6 : recompense -1\n",
    "\n",
    "\n",
    "<h5>REGLES</h5>\n",
    "\n",
    "- Le taxi se déplace selon les actions indiquées.\n",
    "\n",
    "- On ne peut pas se déplacer à travers un mur.\n",
    "\n",
    "- Le jeu est fini quand le bon passager est déposé à la bonne déstination.\n",
    "\n",
    "Plus de détail en anglais : \n",
    "https://www.gymlibrary.dev/environments/toy_text/taxi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Indications sur le code</h3>\n",
    "Choisir une nouvelle action de manière aléatoire : \n",
    "\n",
    "    `action = env.action_space.sample()`\n",
    "    \n",
    "Choisir la meilleure action pour un état donné `s`: \n",
    "\n",
    "    Choisir la plus grande valeur pour `Q[s]`\n",
    "\n",
    "Prendre une action avec gym :\n",
    "\n",
    "    `obs, reward, done, truncated, info = env.step(action)`\n",
    " \n",
    " `obs`: nouvel état, \n",
    " `reward`: recompense obtenue, \n",
    " `done`: boolean, indiquant si le jeu est fini, \n",
    " `truncated`: boolean, indique si le jeu est fini suite à une action irrégulière (pas applicable dans ce cas).\n",
    " `info`, diverses infos sur l'action\n",
    "\n",
    "Choisir la plus grande valeur d'un tableau T: \n",
    "\n",
    "    `max_val = np.argmax(T)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('Taxi-v3')\n",
    "env.reset()\n",
    "\n",
    "NUM_ACTIONS = env.action_space.n # nombre des actions \n",
    "NUM_STATES = env.observation_space.n # nombre des états\n",
    "\n",
    "# Tablea dans lequel on va stocker les valeurs de l'action; dimension NUM_STATES * NUM_ACTIONS\n",
    "Q = np.zeros([NUM_STATES, NUM_ACTIONS]) \n",
    "\n",
    "gamma = 0.95 # facteur d'actualisation (discount factor)\n",
    "alpha = 0.1 # taux d'apprentissage (learning rate)\n",
    "epsilon = 0.1 # facteur pour epsilon greedy (approche glutonne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Entraînement du modèle</h3>  \n",
    "<br/>\n",
    "Pour l'entraînement on doit remplir le tableau Q, qui où on a une valeur pour chaque état et chaque action correspondante.\n",
    "<br/><br/>\n",
    "Dans l'étape d'entraînement, on joue le jeu plusieurs fois (boucle for), et à chaque jeu on passe d'une état à un autre (boucle while). Dans la boucle while, on a le couple [etat - action] courants et [etat - action] suivants. Avec ces éléments et la recompense, on met à jour, au fur et à mesure, le modèle, notamment les cases correspondantes dans le tableau Q, avec l'algorithme QLearning (voir cours 3).\n",
    "<br/><br/>\n",
    "1. On a besoin donc des couples :\n",
    "<br/>\n",
    "(état courant - action courante) : (s, a)\n",
    "<br/>\n",
    "A partir d'un état `s`, on choisit l'action `a` avec la politique epsilon greedy \n",
    "<br/>\n",
    "<br/>\n",
    "2. (état suivant - action suivante) : (s_, a_)\n",
    "<br/>\n",
    "<br/>\n",
    "A partir de l'état `s`, en prenant l'action `a`, on arrive dans l'état suivant `s_`. Il nous faut maintenant l'action `a_` correspondante. Avec l'algorithme QLearning, cette action est la meilleure action de l'état `s_`. On la trouve dans le tableau Q ou pour chaque état et pour chaque action on a des valeurs (on prend la valeur la plus élevée pour l'état `s_`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5000 a la recompense moyenne: 1.0\n",
      "Taxi a été résolu !\n"
     ]
    }
   ],
   "source": [
    "for episode in range(1,500001):\n",
    "    # boolean qui indique si on a fini le jeu on non : \n",
    "    # False si on n'a pas fini le jeu\n",
    "    # True si on a  fini le jeu\n",
    "    done = False \n",
    "    obs, _ = env.reset() \n",
    "    c = 0\n",
    "    rng = np.random.default_rng() # Genere un nombre random entre zéro et 1\n",
    "    while done != True: # tant qu'on n'a pas fini notre jeu :\n",
    "        # on choisit l'action avec la méthode epsilon greedy (approche glutonne) :\n",
    "        # on choisit une action aléatoire pour une probabilité epsilon des cas et \n",
    "        # on choisit la meilleure action pour (1 - epsion) cas\n",
    "        \n",
    "        # ici, on va générer un numéro aléatoire entre 0 et 1 : \n",
    "        random_num = rng.random()\n",
    "        if random_num < epsilon: # exploration : on choisit une action aléatoire\n",
    "            action = env.action_space.sample() # genere une action aléatoire\n",
    "        else: # exploitation : on choisit la meilleure action pour l'état présent : obs\n",
    "            action = np.argmax(Q[obs])  # Parmis les actions effectuable on prend la meilleur\n",
    "            \n",
    "        # On prend une action :     \n",
    "        obs2, rew, done, truncated, info = env.step(action)  # Execute l'action qu'on a choisit avec l'epsilon greedy\n",
    "        \n",
    "        # On met à jour la matrice Q(s, a) avec la formule Q-learning (voir cours) :\n",
    "        Q[obs,action] += alpha * (rew + gamma * (Q[obs2, np.argmax(Q[obs2])] - Q[obs,action]))\n",
    " # A COMPLETER !!!\n",
    "        \n",
    "        # On se positionne sur le nouvel état:\n",
    "        obs = obs2   \n",
    "        \n",
    "    if episode % 5000 == 0:\n",
    "        #à chaque 5000 pas, testons 100 jeux afin d'obtenir la moyenne des retours et \n",
    "        # vérifier si l'apprentissage automatique est fini\n",
    "        \n",
    "        rew_average = 0.\n",
    "        for i in range(100):\n",
    "            obs, _ = env.reset()\n",
    "            done=False\n",
    "            while done != True: \n",
    "                action = np.argmax(Q[obs]) # on choisit l'action maximum pour l'état obs:\n",
    "                obs, rew, done, truncated, info = env.step(action) # prend l'action séléctionée\n",
    "                if done is True and rew == 20:\n",
    "                    rew_average += 1\n",
    "        rew_average=rew_average/100\n",
    "        print('Episode {} a la recompense moyenne: {}'.format(episode,rew_average))\n",
    "        \n",
    "        if rew_average > 0.8: \n",
    "            # On défini le jeu FrozenLake-v0 fini \n",
    "            # si on a la recompense moyenne de 80 sur 100 essais\n",
    "            print(\"Taxi a été résolu !\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Récompense: 6.0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# On initialise le jeu avec les paramètres qui permettent d'afficher un jeu:\n",
    "env = gym.make('Taxi-v3', render_mode=\"human\")\n",
    "\n",
    "# Voyons maintenant comment notre problème est résolu, après avoir amélioré la politique :\n",
    "G = 0.\n",
    "obs, _ = env.reset()\n",
    "done=False\n",
    "while done != True: \n",
    "    # On choisit la meilleure action pour l'état:\n",
    "    action = np.argmax(Q[obs])\n",
    "    \n",
    "    # On prend cette action :\n",
    "    obs, rew, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # On ajoute la recompense au retour (recompense totale de l'episode)\n",
    "    G += rew\n",
    "    \n",
    "# On affiche les étapes du jeu : \n",
    "env.render()\n",
    "\n",
    "# On affiche la recompense:\n",
    "print(\"Récompense:\", G)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "renforced-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "43c7dff35cef67913a40e63d7aca024d4e0aea69e5e52d46956374b9b0243c99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
